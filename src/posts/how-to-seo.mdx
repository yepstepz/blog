---
title: "Ошибки SEO 101: засовываем сайт в поисковик"
date: '2023-12-1'
description: 'Моего сайта не было в поисковой выдаче Google и Yandex. Я зафейлила индексацию, или индексация
зафейлила всех нас? Что делать с Discovered – not indexed? Что такое Indexing API? Это, и многое другое – внутри.'
tags: ["tech", "me"]
published: true
---

«Ты будешь привлекать аудиторию в блог через органику?» – как-то раз меня спросил коллега. «Да, как минимум», – 
неуверенно ответила я. 

Органика – это когда люди приходят к тебе на сайт со страниц поисковика.

Когда я полезла проверять присутствие своего блога в Гугле и Яндексе, я не нашла там ничего, кроме начальной страницы.

Ожидаемо. Никто не ссылался на мой блог из «Внешнего Интернета». Пришло время немного поработать над SEO, благо, 
я знала с чего начинать.

Спустя пару недель я полезла посмотреть выхлоп от своей работы.

Ничего нового.

0 новых проиндексированных страниц.

<Gallery images={[
    {
      fill: true,
      aspectRatio: "12/5",
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/1.webp",
      alt: "На картинке Оби Ван Кеноби не может найти информацию"
    }
  ]}
/>

В чем проблема и как я с ней боролась, расскажу дальше.

## Пререквизиты – с чего начинала
 
У меня статический блог на Next.js, я получаю готовую HTML-верстку. Также я изначально добавила title и description[^1] в 
каждый пост. Кроме этого, организовала адаптивную верстку[^2], чтобы статьи легко открывались с мобильного устройства.

Сюда же добавила <a rel="noreferrer nofollow noopener" target="_blank" href="https://ogp.me/">микроразметку</a>, чтобы посты 
хорошо выглядели при размещении ссылки, например, в Телеграм-канале.

Решила пропустить шаги с созданием <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview">Sitemap</a> 
и <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/robots/intro">robots.txt</a>, 
так как не была уверена, что это актуально для такого маленького ресурса.

Моя страница уже была проиндексирована – я ссылаюсь на нее в своем гитхабе.

Так как никто никогда не упоминал мой блог в Интернете, кроме меня в моем телеграм-канале, я решила самостоятельно сообщить
поиску Google что у меня теперь много новых страниц.

## Как попасть в Google?

В <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/fundamentals/how-search-works#crawling">статье</a> о поисковом движке Google
есть раздел о том, как страницы вообще попадают в выдачу. Ларри Пейдж и Сергей Брин визуализировали страницы интернета 
как большой граф[^3], который связан ссылками между собой. Соответственно, чтобы положить
страничку в поисковик, надо чтобы бот нашел на неё ссылку.

Ссылки ищут специальные боты – краулеры. Они сканируют уже известные странички и добавляют новые ссылки в некую очередь.

Кто запустил краулеров на первые страницы? Про это есть увлекательная статья 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://www.wired.com/2005/08/battelle/">The Birth Of Google</a>, 
в которой рассказывается, как сайт Стенфорндского Университета стал первой ссылкой, которую нашел Google, а дальше все строилось 
на том, что самые важные ресурсы – обычно самые цитируемые.

Могут ли краулеры воспринимать SPA без SSR[^4]? Какие-то могут. Сейчас мы рассматриваем Гугл в качестве примера,
и у них есть целая 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/javascript/javascript-seo-basics">страничка</a> 
с советами. А вот у Яндекса краулинг страниц, срендеренных JavaScript'ом, пока в бете.

Я открыла <a rel="noreferrer nofollow noopener" target="_blank" href="https://search.google.com/search-console/">Google Search Panel</a>, 
связала свой домен с учеткой с помощью TXT-записи в DNS и закинула парочку URL'ов на переиндексацию.
Вот-вот меня должны были навестить краулеры.

Однако, спустя несколько дней, ничего не изменилось.

## Ритуальные танцы для краулеров

Пройдя через отрицание-гнев-депрессию, я решила обратиться к 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/">документации</a>
Гугла.

Я положила в корень сайта Sitemap. Тем более, в 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview">гайде</a> Google 
прямо указано, если ресурс *новый*, малосвязанный внутри, можно помочь краулерам обнаружить контент быстрее.

<a rel="noreferrer nofollow noopener" target="_blank" href="https://www.sitemaps.org/protocol.html">Протокол</a> довольно очевидный. 
Примерная схема карты сайта выглядит так:

```sitemap.xml
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>http://example.com</loc>
    <lastmod>new Date(...)</lastmod>
  </url>
</urlset>
```

Ссылку на `sitemap.xml` я разместила в файле <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/robots/intro">robots.txt</a>. 
Это тот файл, который поисковые роботы парсят в первую очередь. Так они узнают, какие страницы лучше не навещать, чтобы не создавать 
лишнюю нагрузку на сервер.

```robots.txt
Sitemap: http://example.com

User-agent: *
Allow: /
```

На каждую страницу я также добавила 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/docs/crawling-indexing/consolidate-duplicate-urls">каноническую ссылку</a>.
Вероятно, когда я прикручу к блогу поиск или пагинацию, роботы будут пытаться просканировать параметризованные страницы как новые.
Нужно дать ботам понять, что это не дубли, а те самые уже знакомые страницы.

```Layout.jsx
  ...
  {
    url &&  <link rel="canonical" href={url} />
  }
  ...
```

Я заметила, что персональная панель поиска Гугла обновляется примерно раз в четыре дня. Cкормила ей ссылку на карту сайта и
села ожидать.

Спустя 4 дня я увидела изменения в графике.

<Gallery images={[
    {
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/2.png",
      alt: "На картинке Google нашел страницы, но не проиндексировал их."
    }
  ]}
/>

Все новые страницы получили статус «Обнаружена, не проиндексирована», в переводе: «Discovered – not indexed».
Роботы навестили мою карту сайта и считали оттуда ссылки. И решили, что им туда не нужно. 

## Переключаем внимание

На сайте Google описывается только одна причина такого статуса у страниц:

>Это сообщение означает, что мы нашли страницу, но пока не добавили ее в индекс Google. 
>
>Обычно это объясняется тем, что роботу Google не удалось просканировать сайт, 
>поскольку это могло привести к чрезмерной загрузке ресурса, и сканирование было перенесено на более поздний срок.

Якобы роботы подумали и поняли что не хотят нагружать
мой сайт, и он молодой, и надо подождать.

Однако, 23 ноября этому блогу исполнился *год*. А значит, молодость ресурса под вопросом.

Я решила впасть в режим ожидания на недельку-другую и навестить пока отечественный поисковик – Яндекс.

## Вебмастер Яндекса

У Яндекса есть собственная панель для владельцев сайта – <a rel="noreferrer nofollow noopener" target="_blank" href="https://webmaster.yandex.ru/">Я.Вебмастер</a>. По аналогии, 
я подключила домен через TXT-запись и скормила сайту карту блога.

Также, в панели можно предложить урлы для сканирования целой пачкой. Так я и поступила.

Десять лет назад Яндекс был впереди планеты всей по сканированию русскоязычного контента, и в этот раз я понадеялась, 
что краулеры родного поисковика не так нагружены работой, как у Гугла.

На следующий день после принудительного добавления страниц в Вебмастер, все они были просканированы. Я ликовала.
По крайней мере, в будущем трафик будет приходить отсюда.

Однако, через день, так же быстро, Яндекс пометил все словами «Малоценная или маловостребованная страница».

## Контент виноват?

После изучения официальных источников, реддита, каждого SEO-форума, я все еще не понимала смысла пометки 
«Discovered – not indexed» и «Малоценная или маловостребованная страница».

С одной стороны, Гугл и Яндекс в один голос рассказывают, что не все страницы должны быть в поиске. Только 
лучшие из лучших. Пишите лучше, интереснее и релеватнее, говорили они.

<Gallery images={[
    {
      aspectRatio: "16/9",
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/democracy.jpg",
      alt: "Сенатор на картинке говорит, что любит демократию"
    }
  ]}
/>

Контент должен быть человечным и уникальным. Так ли это? Такое я уже слышала на своих старых работах, где мне приходилось писать 
тонны текста, который был заточен под поисковики.

## Как я работала на SEO-компанию

В студенческие годы мне не хватало денег, поэтому я хваталась за разные подработки. 

Больше всего меня впечатлило копирайтерство на бирже Адвего. Ты не видишь людей, люди не видят тебя, сиди и пиши что угодно.
Технические задания дотошно указывали, сколько ключей[^5] нужно вставить в текст и в каком количестве.

После 2014 года я закончила бакалавриат и вошла в большой мир поиска работы. Навыков у меня было не так много: теоретические знания 
с универа, какие-то нишевые случайные халтурки и ОН – КОПИРАЙТЕРСКИЙ СТАЖ.

Я оформила резюме и пару лет проработала на разные компании, занимаясь SEO-копирайтингом и контекстной рекламой.

На первой работе после универа, с 10:00 до 19:00, я постоянно редактировала чужие копирайтерские тексты. Моей задачей было
прочесать все на предмет перенасыщенности одинаковыми фразами, исправить ошибки и проверить уникальность.

<Gallery images={[
    {
      aspectRatio: "16/9",
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/granny.jpeg",
      alt: "Бабушка видит типичные ошибки в структуре верстки страницы"
    }
  ]}
/>

Для генерации ключевых фраз использовался, например, 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://wordstat.yandex.ru/">Wordstat</a>. Он работает и сейчас,
хотя дизайн там довольно старый.

Какие-то сайты росли как на дрожжах. Другие – открывали судебные разбирательства с нашей компанией. В общем, было весело.

Никто не имел четкого представления, как поисковики ранжируют сайты.
Почему так? Ответ прост: поисковики не хотят, чтобы кто-то использовал выдачу во имя собственных интересов.

## Искусственный Интеллект, продаваны и Google

Немного о продаванах: весь конец ноября в твиттере вирусился парень, который предлагал решение о быстром заработке с использованием AI.
Его способ:

- найти карту сайта соперника (желательно с человекопонятными URL);
- спарсить из списка названия статей;
- загнать их в AI-генератор статей <a rel="noreferrer nofollow noopener" target="_blank" href="https://byword.ai/">Byword</a>;
- нагенерировать сотни статей на темы из списка;
- запостить их на своем сайте.

Результат он описывает в своем аккаунте на скринах: траффик стремительно растет вверх.

<Gallery images={[
    {
      aspectRatio: '2/1',
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/3.png",
      alt: "На картинке пользователь твиттера показывает бешеный рост траффика на сайты со сгенерированными текстами."
    }
  ]}
/>

Спустя неделю, Google наконец-то пессимизировал сайт в выдаче по многочисленным жалобам.

<Gallery images={[
    {
      aspectRatio: '2/1',
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/4.png",
      alt: "На картинке пользователь твиттера показывает, что гугл пессимизировал траффик на сайт со сгенерированными текстами."
    }
  ]}
/>

Каждый раз, когда оптимизаторские гипотезы подтверждаются и начинают применяться массово, пострадавший
поисковик начинает тюнить свои алгоритмы, чтобы подлечить выдачу.

## Самая темная ночь

Конечно, инстиктивный порыв – прогнать свои тексты через инструмент и посмотреть, какие ключевые слова можно туда всунуть.

Но я так не хочу.

Мои первые посты в блоге были очень короткие, но сейчас-то я пишу намного больше. Я решила поресерчить, что пишут 
пользователи реддита и разработчики поиска.

Я проштудировала все советы из интернета: 

- проанализировала через Google Search Panel возможность индексации;
- проверила лайтхаусом, все ли в порядке со скоростью загрузки;
- посмотрела, как роботы видят страницу в разных разрешениях;
- проверила отображение метатегов на всех страницах.

Все ок; Все индексируемо. Как будто мой ресурс пока все еще молод для индексации.

В 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://blog.google/products/search/how-ai-powers-great-search-results/">статье</a> 2022 года Pandu Nayak рассказывает
о том, какие успехи случились после внедрения AI в алгоритмы Google. А успехи такие: теперь поисковая машина понимает не только
прямой смысл написанного, но и контекст, намерения и даже тон.

Но в то же время, в интернете полно страниц на которых информация повторяется/сгенерирована с помощью AI, 
как в примере выше.

Чем я хуже? Стоит ли просто задвинуть в далекий угол идею с продвижением через органику?

Почти тогда же я обнаружила Google Indexing API.

## Перед рассветом – Google Indexing API

Краулеры парсят сайт, но для это им требуются серверные мощности. Поэтому Google релизнул специальное 
<a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/apis/indexing-api/v3/quickstart">API</a>, через
которое можно сообщать поиску, что у тебя появилось кое-что новенькое.

Подобное <a rel="noreferrer nofollow noopener" target="_blank" href="https://webmaster.yandex.ru/blog/kak-bystro-rasskazat-yandeksu-ob-izmeneniyakh-na-sayte">API появилось</a>
также и у Яндекса.

Однако, Google Indexing API предназначено для постинга трансляций и предложений о работе.

То есть, для контента, который быстро устаревает.

Обычные пользователи с реддита советуют использовать именно его для быстрого индексирования сайта, несмотря на ограничения.

Я решила попробовать проиндексировать с его помощью несколько страниц, чтобы посмотреть на результаты.

### Настраиваем

В <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/apis/indexing-api/v3/prereqs#node.js">гайде</a> по настройке API есть информация,
как получить JSON-ключ сервис-аккаунта, от имени которого будет делаться запрос.

Если кратко:

- создаем сервис-аккаунт в <a rel="noreferrer nofollow noopener" target="_blank" href="https://console.cloud.google.com/">облаке</a> Google как JSON-файл;
- скачиваем его на свой компьютер;
- присваиваем сервис-аккаунту права Создателя через Google Search Console;
- пишем небольшой node-скрипт, который будет использовать данные из JSON-файла.

Пишем получение ключа:

```index.js
const jwtClient = new google.auth.JWT(
  key.client_email,
  null,
  key.private_key,
  ["https://www.googleapis.com/auth/indexing"],
  null
);

jwtClient.authorize(function(err, tokens) {
  if (err) {
    console.log(err);
    return;
  }
  // ... здесь будем посылать ключ
});

```

В оригинальном гайде показано, как отправить единичную ссылку. Однако, для чистоты эксперимента, мне необходимо было
отправить пачку ссылок. Я подсмотрела <a rel="noreferrer nofollow noopener" target="_blank" href="https://github.com/swalker-888/google-indexing-api-bulk/blob/master/index.js">решение</a>
с отправкой ссылок с помощью ```multipart```.

Вот что мы кладем внутрь блока с авторизацией:

```index.js
  jwtClient.authorize(function(err, tokens) {
    if (err) {
      console.log(err);
      return;
    }
  
    const items = batch.map(line => {
      return {
        'Content-Type': 'application/http',
        'Content-ID': '',
        body:
          'POST /v3/urlNotifications:publish HTTP/1.1\n' +
          'Content-Type: application/json\n\n' +
          JSON.stringify({
            url: line,
            type: 'URL_UPDATED'
          })
      };
    });
  
    const options = {
      url: 'https://indexing.googleapis.com/batch',
      method: 'POST',
      headers: {
        'Content-Type': 'multipart/mixed'
      },
      auth: { bearer: tokens.access_token },
      multipart: items
    };
    request(options, (err, resp, body) => {
      console.log(body);
    });
  });
```

Запускаем команду в консоли:

```shell
  node index.js
```

Если скрипт вернул ошибку (скорее всего, это будет 403), значит нужно вернуться к <a rel="noreferrer nofollow noopener" target="_blank" href="https://developers.google.com/search/apis/indexing-api/v3/prereqs#node.js">гайду</a>
и проверить настройку прав сервис-аккаунта.

После того, как я получила ответ 200, через несколько минут я направилась в Google Search Panel смотреть на результаты.

### Новый статус

Страницы, которые я кинула в Google Indexing API изменили свой статус с *Discovered – not indexed* на *Crawled – not Indexed*.

<Gallery images={[
    {
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/5.jpg",
      alt: "Уолтер Вайт недоволен работой краулеров"
    }
  ]}
/>

Это значит, что краулеры наконец-то посетили мои ссылки, а не просто обнаружили их наличие. Но решили не класть его в индекс. СНОВА.

После быстрого поиска по интернету, я опять прочитала те же самые статьи про улучшение качества контента, что контент нужно 
оптимизировать не под движки поиска, а под человека, и так далее, и тому подобное.

На этой ноте я сдалась и пошла спать.

И на утро обнаружила долгожданную зеленую галочку около тех трех страниц, что я закинула в сервис.

<Gallery images={[
    {
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/6.png",
      alt: "Уолтер Вайт недоволен работой краулеров"
    }
  ]}
/>

И, что самое интересное, некоторые страницы моего блога на следующий день заехали в индекс Яндекса. Совпадение? 
Или, если твои «малоинтересные» страницы интересны Гуглу, то не такие уж они и «малоинтересные».

<Gallery images={[
    {
      fill: true,
      style: { 
        objectFit: "contain"
      },
      src: "/how-to-seo/7.png",
      alt: "Уолтер Вайт недоволен работой краулеров"
    }
  ]}
/>

## Выводы

После нескольких недель упорной борьбы с поисковиками мне все-таки удалось добиться первых результатов.
Какие-то рекомендации от меня:

- провести техническую работу над сайтом – как в <a href="https://developers.google.com/search/docs/crawling-indexing/">гайде</a> Google;
- связать сайт с <a rel="noreferrer nofollow noopener" target="_blank" href="https://search.google.com/search-console/">Google Search Panel</a> и <a rel="noreferrer nofollow noopener" target="_blank" href="https://webmaster.yandex.ru/">Я.Вебмастер</a>;
- продолжать создавать контент;
- в крайних случаях использовать Google Indexing API.

Конечно, работы еще много, мой собственный контент в блоге далек от идеала. 

Что нужно пофиксить:

- добавить все разделы сайта в sitemap.xml;
- дополнить карту полями из <a rel="noreferrer nofollow noopener" target="_blank" href="https://www.sitemaps.org/protocol.html">протокола</a>;
- добавить в статьи ссылки на предыдущие статьи, чтобы повысить внутреннюю связность;
- сделать 301 редирект с */articles на */posts 😮‍💨;
- сделать корневую страницу для тегов.

Наверное, нет смысла медитировать над статистикой индексации. Всему свое время.

На этом все, удивимся в следующем посте.



_____

[^1]: `<title/>` и `<meta name="description" content="***">` – базовые теги, которые использует поисковик для построение
сниппетов поиска. `<title/>` также используется в названии вкладки.

[^2]: Адаптивная верстка - верстка для разных размеров экрана, планшета и телефона.

[^3]: Граф – узлы (точки), соединенные ребрами (линиями). Простой пример графа: карта метро.

[^4]: SPA – Single Page Application, приложение, которое динамически перестраивает контент внутри себя. Вся логика
происходит внутри JS-файлов, а точка входа – один единственный HTML-файл. В классических сайтах точек входа много.
SSR – Server Side Rendering, сервер принимает логику из JS и выплевывает HTML-код прямо во время использования сайта.

[^5]: Ключи/keywords – повторяющиеся словосочетания на определенную тему.